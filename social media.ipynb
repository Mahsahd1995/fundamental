{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange; text-align:left;size:10\">User Behavior Analysis for Optimizing Engagement on Social Media Platforms </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to inspect its structure and contents\n",
    "file_path = \"Social_Media_User_Activity_Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set plot style\n",
    "plt.style.use(\"ggplot\")\n",
    "# Analyze overall engagement metrics\n",
    "engagement_metrics = [\"Posts_Created\", \"Comments_Made\", \"Messages_Sent\", \"Likes_Given\", \"Shares_Made\"]\n",
    "engagement_data = df[engagement_metrics].sum()\n",
    "# Plot engagement distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=engagement_data.index, y=engagement_data.values)\n",
    "plt.xlabel(\"Engagement Metrics\")\n",
    "plt.ylabel(\"Total Count\")\n",
    "plt.title(\"Overall User Engagement on Social Media Platform\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "# Display engagement statistics\n",
    "df_engagement_stats = df[engagement_metrics].describe()\n",
    "print(\"Engagement Statistics:\\n\", df[engagement_metrics].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "# Check for statistical outliers using basic descriptive statistics\n",
    "summary_statistics = df.describe()\n",
    "missing_values, summary_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Average engagement per content type\n",
    "avg_engagement_by_content = df.groupby(\"Preferred_Content_Type\")[[\"Posts_Created\", \"Comments_Made\", \"Messages_Sent\", \"Likes_Given\", \"Shares_Made\"]].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "avg_engagement_by_content.plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.xlabel(\"Preferred Content Type\")\n",
    "plt.ylabel(\"Average Engagement Metrics\")\n",
    "plt.title(\"Average Engagement Metrics by Preferred Content Type\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title=\"Engagement Metrics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bar Chart: Comparing user activity metrics across different user categories\n",
    "activity_metrics = ['Posts_Created', 'Comments_Made', 'Messages_Sent', 'Likes_Given', 'Shares_Made']\n",
    "df_grouped = df.groupby('User_Category')[activity_metrics].mean()\n",
    "df_grouped.plot(kind='bar', figsize=(10, 6), edgecolor='black')\n",
    "plt.title(\"Average User Activity Metrics by User Category\")\n",
    "plt.xlabel(\"User Category\")\n",
    "plt.ylabel(\"Average Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Activity Type\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload necessary libraries after execution state reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# حذف داده‌های تکراری\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# جایگزینی داده‌های گم‌شده (در صورت وجود)\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)  # استفاده از میانه برای داده‌های عددی\n",
    "\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)  # جایگزینی مقدار پرتکرار برای داده‌های دسته‌بندی‌شده\n",
    "\n",
    "# انتخاب ستون‌های عددی برای بررسی نقاط پرت\n",
    "columns_to_check = [\"Posts_Created\", \"Comments_Made\", \"Messages_Sent\", \n",
    "                    \"Likes_Given\", \"Shares_Made\", \"Time_Spent_per_Day (minutes)\"]\n",
    "\n",
    "# شناسایی نقاط پرت با استفاده از روش IQR (Interquartile Range)\n",
    "Q1 = df[columns_to_check].quantile(0.25)\n",
    "Q3 = df[columns_to_check].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# حذف نقاط پرت (داده‌هایی که خارج از 1.5 برابر IQR هستند)\n",
    "df_cleaned = df[~((df[columns_to_check] < (Q1 - 1.5 * IQR)) | (df[columns_to_check] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# نمایش نمونه‌ای از داده‌های تمیز شده\n",
    "df_cleaned.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange; text-align:left;size:10\">missing value </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print(\"Columns in dataset before processing:\\n\", df.columns)\n",
    "df.columns = df.columns.str.strip()\n",
    "#missing value\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values before imputation:\\n\", missing_values)\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "if 'Time_Spent_per_Day' in df.columns and 'Time_Spent_per_Day (minutes)' in df.columns:\n",
    "    df['Time_Spent_per_Day'].fillna(df['Time_Spent_per_Day (minutes)'].mean(), inplace=True)\n",
    "elif 'Time_Spent_per_Day (minutes)' in df.columns:\n",
    "    df.rename(columns={'Time_Spent_per_Day (minutes)': 'Time_Spent_per_Day'}, inplace=True)\n",
    "    df['Time_Spent_per_Day'].fillna(df['Time_Spent_per_Day'].mean(), inplace=True)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "if 'Time_Spent_per_Day' in df.columns and 'Likes_Given' in df.columns:\n",
    "    df[['Time_Spent_per_Day', 'Likes_Given']] = imputer.fit_transform(df[['Time_Spent_per_Day', 'Likes_Given']])\n",
    "\n",
    "# (Outliers)IQR\n",
    "if 'Time_Spent_per_Day' in df.columns:\n",
    "    Q1 = df['Time_Spent_per_Day'].quantile(0.25)\n",
    "    Q3 = df['Time_Spent_per_Day'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df['Time_Spent_per_Day'] < (Q1 - 1.5 * IQR)) | \n",
    "              (df['Time_Spent_per_Day'] > (Q3 + 1.5 * IQR)))]\n",
    "# (Scaling)\n",
    "scaler = MinMaxScaler()\n",
    "if 'Time_Spent_per_Day' in df.columns and 'Likes_Given' in df.columns:\n",
    "    df[['Time_Spent_per_Day', 'Likes_Given']] = scaler.fit_transform(df[['Time_Spent_per_Day', 'Likes_Given']])\n",
    "\n",
    "print(\"\\n✅ Data processing completed successfully!\")\n",
    "print(\"Columns in dataset after processing:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange; text-align:left;size:10\">confusion_matrix </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])  \n",
    "y_pred_rf = np.array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0])  # Random Forest\n",
    "y_pred_svm = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])  #ط SVM\n",
    "\n",
    "cm_rf = confusion_matrix(y_true, y_pred_rf)\n",
    "cm_svm = confusion_matrix(y_true, y_pred_svm)\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Highly Active\", \"Moderately Active\"], yticklabels=[\"Highly Active\", \"Moderately Active\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "# Random Forest\n",
    "plot_confusion_matrix(cm_rf, \"Confusion Matrix - Random Forest\")\n",
    "# SVM\n",
    "plot_confusion_matrix(cm_svm, \"Confusion Matrix - SVM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardizing the dataset again\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[['Posts_Created', 'Comments_Made', 'Messages_Sent', 'Likes_Given', \n",
    "                                    'Shares_Made',  'Active_Days_per_Week']])\n",
    "# Reapplying K-Means clustering with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "# Reapplying DBSCAN clustering with adjusted parameters\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=10)\n",
    "df['DBSCAN_Cluster'] = dbscan.fit_predict(X_scaled)\n",
    "# Applying PCA for visualization (reducing dimensions to 2D)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# Adding PCA components to the dataframe for visualization\n",
    "df['PCA1'] = X_pca[:, 0]\n",
    "df['PCA2'] = X_pca[:, 1]\n",
    "# Visualizing K-Means Clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x=df['PCA1'], y=df['PCA2'], hue=df['KMeans_Cluster'], palette='viridis', alpha=0.7)\n",
    "plt.title(\"K-Means Clustering Visualization\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "# Visualizing DBSCAN Clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x=df['PCA1'], y=df['PCA2'], hue=df['DBSCAN_Cluster'], palette='tab10', alpha=0.7)\n",
    "plt.title(\"DBSCAN Clustering Visualization\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange; text-align:left;size:10\">User Behavior Analysis for Optimizing Engagement on Social Media Platforms </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the correct column name for Time Spent per Day is used\n",
    "correct_numeric_cols = ['Posts_Created', 'Comments_Made', 'Messages_Sent', 'Likes_Given', \n",
    "                        'Shares_Made', 'Time_Spent_per_Day (minutes)', 'Active_Days_per_Week']\n",
    "# Ensuring all numeric columns are properly converted\n",
    "df[correct_numeric_cols] = df[correct_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "# Calculate IQR for Outlier Detection\n",
    "Q1 = df[correct_numeric_cols].quantile(0.25)\n",
    "Q3 = df[correct_numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Identifying outliers\n",
    "outliers = ((df[correct_numeric_cols] < (Q1 - 1.5 * IQR)) | (df[correct_numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
    "# Summary statistics\n",
    "summary_stats = df[correct_numeric_cols].describe()\n",
    "# Visualizing distributions of numerical columns\n",
    "df[correct_numeric_cols].hist(bins=20, figsize=(12, 8), grid=False, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Distribution of Numerical Features\", fontsize=16)\n",
    "plt.show()\n",
    "# Boxplot for detecting outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[correct_numeric_cols], orient='h', palette=\"coolwarm\")\n",
    "plt.title(\"Boxplot of Numerical Features\")\n",
    "plt.show()\n",
    "# Display engagement statistics\n",
    "summary_stats, outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Preferred_Content_Type\"] = label_encoder.fit_transform(df[\"Preferred_Content_Type\"])\n",
    "df[\"User_Category\"] = label_encoder.fit_transform(df[\"User_Category\"])  # Target variable encoding\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=[\"User_ID\", \"User_Category\"])\n",
    "y = df[\"User_Category\"]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Train SVM Classifier\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "svm_acc = accuracy_score(y_test, svm_preds)\n",
    "rf_report = classification_report(y_test, rf_preds)\n",
    "svm_report = classification_report(y_test, svm_preds)\n",
    "\n",
    "rf_acc, svm_acc, rf_report, svm_report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric values for correlation analysis\n",
    "df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = df_encoded.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical variables using label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_encoded[col] = label_encoders[col].fit_transform(df_encoded[col])\n",
    "\n",
    "# Compute correlation matrix again with numerical values\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of User Activity Data (Encoded)\")\n",
    "plt.show()\n",
    "\n",
    "# Display the first few rows of the encoded dataset to verify changes\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numerical columns for analysis\n",
    "numeric_cols = ['Posts_Created', 'Comments_Made', 'Messages_Sent', 'Likes_Given', \n",
    "                'Shares_Made', 'Time_Spent_per_Day', 'Active_Days_per_Week']\n",
    "\n",
    "# Ensure all numeric columns are properly converted \n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate IQR for Outlier Detection\n",
    "Q1 = df[numeric_cols].quantile(0.25)\n",
    "Q3 = df[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifying outliers\n",
    "outliers = ((df[numeric_cols] < (Q1 - 1.5 * IQR)) | (df[numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "# Display summary statistics\n",
    "summary_stats = df[numeric_cols].describe()\n",
    "\n",
    "# Visualizing distributions of numerical columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[numeric_cols].hist(bins=20, figsize=(12, 8), grid=False, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Distribution of Numerical Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for detecting outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[numeric_cols], orient='h', palette=\"coolwarm\")\n",
    "plt.title(\"Boxplot of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Display engagement statistics\n",
    "summary_stats, outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels and accuracy values\n",
    "models = [\"Random Forest\", \"SVM\"]\n",
    "accuracies = [rf_acc * 100, svm_acc * 100]\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "plt.ylim(80, 100)  # Set y-axis range for better visualization\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Model Comparison: Random Forest vs SVM\")\n",
    "plt.text(0, rf_acc * 100 + 0.5, f\"{rf_acc * 100:.1f}%\", ha='center', fontsize=12)\n",
    "plt.text(1, svm_acc * 100 + 0.5, f\"{svm_acc * 100:.1f}%\", ha='center', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset to ensure correctness\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check column names\n",
    "df.columns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Drop User_ID as it's not needed for modeling\n",
    "df = df.drop(columns=['User_ID'])\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in ['Preferred_Content_Type', 'User_Category']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = df.drop(columns=['User_Category'])\n",
    "y = df['User_Category']\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize numerical features for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_preds = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate models\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "svm_accuracy = accuracy_score(y_test, svm_preds)\n",
    "\n",
    "rf_report = classification_report(y_test, rf_preds, target_names=label_encoders['User_Category'].classes_)\n",
    "svm_report = classification_report(y_test, svm_preds, target_names=label_encoders['User_Category'].classes_)\n",
    "\n",
    "rf_accuracy, svm_accuracy, rf_report, svm_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrices\n",
    "rf_cm = confusion_matrix(y_test, rf_preds)\n",
    "svm_cm = confusion_matrix(y_test, svm_preds)\n",
    "\n",
    "# Plot confusion matrix for Random Forest\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(rf_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoders['User_Category'].classes_, yticklabels=label_encoders['User_Category'].classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for SVM\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(svm_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoders['User_Category'].classes_, yticklabels=label_encoders['User_Category'].classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - SVM\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
